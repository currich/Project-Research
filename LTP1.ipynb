{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Imported\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import math\n",
    "import matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "from external_library import StockDataset, StockPriceDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", '#FF7D00', '#FF006D', '#ADFF02', '#8F00FF']\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>MA5</th>\n",
       "      <th>MA20</th>\n",
       "      <th>MA60</th>\n",
       "      <th>RSI14</th>\n",
       "      <th>std</th>\n",
       "      <th>upperb</th>\n",
       "      <th>lowerb</th>\n",
       "      <th>%K</th>\n",
       "      <th>%D</th>\n",
       "      <th>SP500</th>\n",
       "      <th>NASDAQ_COMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.675</td>\n",
       "      <td>3.543</td>\n",
       "      <td>3.782250</td>\n",
       "      <td>4.299750</td>\n",
       "      <td>40.927948</td>\n",
       "      <td>0.196449</td>\n",
       "      <td>4.175147</td>\n",
       "      <td>3.389353</td>\n",
       "      <td>40.601489</td>\n",
       "      <td>26.315786</td>\n",
       "      <td>1343.800049</td>\n",
       "      <td>2834.429932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.750</td>\n",
       "      <td>3.588</td>\n",
       "      <td>3.767125</td>\n",
       "      <td>4.281833</td>\n",
       "      <td>45.013998</td>\n",
       "      <td>0.185909</td>\n",
       "      <td>4.138942</td>\n",
       "      <td>3.395308</td>\n",
       "      <td>51.879688</td>\n",
       "      <td>36.090216</td>\n",
       "      <td>1345.020020</td>\n",
       "      <td>2858.830078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.690</td>\n",
       "      <td>3.636</td>\n",
       "      <td>3.754875</td>\n",
       "      <td>4.262042</td>\n",
       "      <td>42.482342</td>\n",
       "      <td>0.182302</td>\n",
       "      <td>4.119478</td>\n",
       "      <td>3.390272</td>\n",
       "      <td>50.000021</td>\n",
       "      <td>47.493733</td>\n",
       "      <td>1337.430054</td>\n",
       "      <td>2842.800049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.600</td>\n",
       "      <td>3.645</td>\n",
       "      <td>3.742250</td>\n",
       "      <td>4.238708</td>\n",
       "      <td>38.944195</td>\n",
       "      <td>0.183921</td>\n",
       "      <td>4.110092</td>\n",
       "      <td>3.374408</td>\n",
       "      <td>35.294105</td>\n",
       "      <td>45.724605</td>\n",
       "      <td>1331.939941</td>\n",
       "      <td>2839.959961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.465</td>\n",
       "      <td>3.636</td>\n",
       "      <td>3.721750</td>\n",
       "      <td>4.214250</td>\n",
       "      <td>34.326053</td>\n",
       "      <td>0.191057</td>\n",
       "      <td>4.103864</td>\n",
       "      <td>3.339636</td>\n",
       "      <td>11.999989</td>\n",
       "      <td>32.431371</td>\n",
       "      <td>1304.890015</td>\n",
       "      <td>2764.790039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Close    MA5      MA20      MA60      RSI14       std    upperb    lowerb  \\\n",
       "0  3.675  3.543  3.782250  4.299750  40.927948  0.196449  4.175147  3.389353   \n",
       "1  3.750  3.588  3.767125  4.281833  45.013998  0.185909  4.138942  3.395308   \n",
       "2  3.690  3.636  3.754875  4.262042  42.482342  0.182302  4.119478  3.390272   \n",
       "3  3.600  3.645  3.742250  4.238708  38.944195  0.183921  4.110092  3.374408   \n",
       "4  3.465  3.636  3.721750  4.214250  34.326053  0.191057  4.103864  3.339636   \n",
       "\n",
       "          %K         %D        SP500  NASDAQ_COMP  \n",
       "0  40.601489  26.315786  1343.800049  2834.429932  \n",
       "1  51.879688  36.090216  1345.020020  2858.830078  \n",
       "2  50.000021  47.493733  1337.430054  2842.800049  \n",
       "3  35.294105  45.724605  1331.939941  2839.959961  \n",
       "4  11.999989  32.431371  1304.890015  2764.790039  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"NVDA_110721_Final_1_fdr.csv\")\n",
    "df = df.drop('Date', axis=1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2264"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(len(df) * .9)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2264, 12), (251, 12))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = df[:train_size], df[train_size+1:]\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>MA5</th>\n",
       "      <th>MA20</th>\n",
       "      <th>MA60</th>\n",
       "      <th>RSI14</th>\n",
       "      <th>std</th>\n",
       "      <th>upperb</th>\n",
       "      <th>lowerb</th>\n",
       "      <th>%K</th>\n",
       "      <th>%D</th>\n",
       "      <th>SP500</th>\n",
       "      <th>NASDAQ_COMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.008116</td>\n",
       "      <td>0.006553</td>\n",
       "      <td>0.008491</td>\n",
       "      <td>0.014538</td>\n",
       "      <td>0.327279</td>\n",
       "      <td>0.022327</td>\n",
       "      <td>0.010340</td>\n",
       "      <td>0.007087</td>\n",
       "      <td>0.406015</td>\n",
       "      <td>0.252223</td>\n",
       "      <td>0.106943</td>\n",
       "      <td>0.059137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.007002</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.014329</td>\n",
       "      <td>0.387794</td>\n",
       "      <td>0.020892</td>\n",
       "      <td>0.009994</td>\n",
       "      <td>0.007156</td>\n",
       "      <td>0.518797</td>\n",
       "      <td>0.352500</td>\n",
       "      <td>0.107476</td>\n",
       "      <td>0.062031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008263</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.008205</td>\n",
       "      <td>0.014098</td>\n",
       "      <td>0.350300</td>\n",
       "      <td>0.020401</td>\n",
       "      <td>0.009808</td>\n",
       "      <td>0.007098</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.469489</td>\n",
       "      <td>0.104158</td>\n",
       "      <td>0.060130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007383</td>\n",
       "      <td>0.007569</td>\n",
       "      <td>0.008072</td>\n",
       "      <td>0.013826</td>\n",
       "      <td>0.297900</td>\n",
       "      <td>0.020621</td>\n",
       "      <td>0.009718</td>\n",
       "      <td>0.006915</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.451339</td>\n",
       "      <td>0.101757</td>\n",
       "      <td>0.059793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.006063</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.007858</td>\n",
       "      <td>0.013540</td>\n",
       "      <td>0.229505</td>\n",
       "      <td>0.021593</td>\n",
       "      <td>0.009659</td>\n",
       "      <td>0.006513</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.314963</td>\n",
       "      <td>0.089929</td>\n",
       "      <td>0.050877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>0.986921</td>\n",
       "      <td>0.999776</td>\n",
       "      <td>0.977557</td>\n",
       "      <td>0.974899</td>\n",
       "      <td>0.699842</td>\n",
       "      <td>0.628450</td>\n",
       "      <td>0.980370</td>\n",
       "      <td>0.974175</td>\n",
       "      <td>0.780552</td>\n",
       "      <td>0.765087</td>\n",
       "      <td>0.917518</td>\n",
       "      <td>0.966967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>0.972277</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983618</td>\n",
       "      <td>0.980833</td>\n",
       "      <td>0.647310</td>\n",
       "      <td>0.624443</td>\n",
       "      <td>0.985343</td>\n",
       "      <td>0.981547</td>\n",
       "      <td>0.701414</td>\n",
       "      <td>0.697269</td>\n",
       "      <td>0.930216</td>\n",
       "      <td>0.974310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>0.963232</td>\n",
       "      <td>0.992545</td>\n",
       "      <td>0.988320</td>\n",
       "      <td>0.987440</td>\n",
       "      <td>0.615381</td>\n",
       "      <td>0.619361</td>\n",
       "      <td>0.988923</td>\n",
       "      <td>0.987600</td>\n",
       "      <td>0.652530</td>\n",
       "      <td>0.712179</td>\n",
       "      <td>0.925411</td>\n",
       "      <td>0.965218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>0.969759</td>\n",
       "      <td>0.987013</td>\n",
       "      <td>0.993464</td>\n",
       "      <td>0.993367</td>\n",
       "      <td>0.630691</td>\n",
       "      <td>0.610222</td>\n",
       "      <td>0.992338</td>\n",
       "      <td>0.994831</td>\n",
       "      <td>0.687805</td>\n",
       "      <td>0.680462</td>\n",
       "      <td>0.929416</td>\n",
       "      <td>0.968700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996146</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.696514</td>\n",
       "      <td>0.622286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.815500</td>\n",
       "      <td>0.719477</td>\n",
       "      <td>0.941270</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2264 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Close       MA5      MA20      MA60     RSI14       std    upperb  \\\n",
       "0     0.008116  0.006553  0.008491  0.014538  0.327279  0.022327  0.010340   \n",
       "1     0.008850  0.007002  0.008333  0.014329  0.387794  0.020892  0.009994   \n",
       "2     0.008263  0.007480  0.008205  0.014098  0.350300  0.020401  0.009808   \n",
       "3     0.007383  0.007569  0.008072  0.013826  0.297900  0.020621  0.009718   \n",
       "4     0.006063  0.007480  0.007858  0.013540  0.229505  0.021593  0.009659   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2259  0.986921  0.999776  0.977557  0.974899  0.699842  0.628450  0.980370   \n",
       "2260  0.972277  1.000000  0.983618  0.980833  0.647310  0.624443  0.985343   \n",
       "2261  0.963232  0.992545  0.988320  0.987440  0.615381  0.619361  0.988923   \n",
       "2262  0.969759  0.987013  0.993464  0.993367  0.630691  0.610222  0.992338   \n",
       "2263  1.000000  0.996146  1.000000  1.000000  0.696514  0.622286  1.000000   \n",
       "\n",
       "        lowerb        %K        %D     SP500  NASDAQ_COMP  \n",
       "0     0.007087  0.406015  0.252223  0.106943     0.059137  \n",
       "1     0.007156  0.518797  0.352500  0.107476     0.062031  \n",
       "2     0.007098  0.500000  0.469489  0.104158     0.060130  \n",
       "3     0.006915  0.352941  0.451339  0.101757     0.059793  \n",
       "4     0.006513  0.120000  0.314963  0.089929     0.050877  \n",
       "...        ...       ...       ...       ...          ...  \n",
       "2259  0.974175  0.780552  0.765087  0.917518     0.966967  \n",
       "2260  0.981547  0.701414  0.697269  0.930216     0.974310  \n",
       "2261  0.987600  0.652530  0.712179  0.925411     0.965218  \n",
       "2262  0.994831  0.687805  0.680462  0.929416     0.968700  \n",
       "2263  1.000000  0.815500  0.719477  0.941270     1.000000  \n",
       "\n",
       "[2264 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(train_df)\n",
    "train_df = pd.DataFrame(\n",
    "    scaler.transform(train_df),\n",
    "    index=train_df.index,\n",
    "    columns=train_df.columns\n",
    ")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>MA5</th>\n",
       "      <th>MA20</th>\n",
       "      <th>MA60</th>\n",
       "      <th>RSI14</th>\n",
       "      <th>std</th>\n",
       "      <th>upperb</th>\n",
       "      <th>lowerb</th>\n",
       "      <th>%K</th>\n",
       "      <th>%D</th>\n",
       "      <th>SP500</th>\n",
       "      <th>NASDAQ_COMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2265</th>\n",
       "      <td>0.992959</td>\n",
       "      <td>0.999392</td>\n",
       "      <td>1.009366</td>\n",
       "      <td>1.011865</td>\n",
       "      <td>0.654377</td>\n",
       "      <td>0.632325</td>\n",
       "      <td>1.009962</td>\n",
       "      <td>1.008637</td>\n",
       "      <td>0.708814</td>\n",
       "      <td>0.730503</td>\n",
       "      <td>0.951844</td>\n",
       "      <td>0.992770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2266</th>\n",
       "      <td>0.962743</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>1.014043</td>\n",
       "      <td>1.017399</td>\n",
       "      <td>0.548284</td>\n",
       "      <td>0.591998</td>\n",
       "      <td>1.008574</td>\n",
       "      <td>1.020645</td>\n",
       "      <td>0.386432</td>\n",
       "      <td>0.583775</td>\n",
       "      <td>0.934195</td>\n",
       "      <td>0.963746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2267</th>\n",
       "      <td>0.969075</td>\n",
       "      <td>0.999153</td>\n",
       "      <td>1.017729</td>\n",
       "      <td>1.022714</td>\n",
       "      <td>0.564638</td>\n",
       "      <td>0.572626</td>\n",
       "      <td>1.009220</td>\n",
       "      <td>1.028001</td>\n",
       "      <td>0.411229</td>\n",
       "      <td>0.497415</td>\n",
       "      <td>0.925437</td>\n",
       "      <td>0.952094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2268</th>\n",
       "      <td>0.991272</td>\n",
       "      <td>0.997376</td>\n",
       "      <td>1.024353</td>\n",
       "      <td>1.028772</td>\n",
       "      <td>0.619646</td>\n",
       "      <td>0.511912</td>\n",
       "      <td>1.006748</td>\n",
       "      <td>1.045618</td>\n",
       "      <td>0.634819</td>\n",
       "      <td>0.472111</td>\n",
       "      <td>0.935835</td>\n",
       "      <td>0.972623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2269</th>\n",
       "      <td>0.971128</td>\n",
       "      <td>0.995125</td>\n",
       "      <td>1.029665</td>\n",
       "      <td>1.034890</td>\n",
       "      <td>0.549769</td>\n",
       "      <td>0.426998</td>\n",
       "      <td>0.999681</td>\n",
       "      <td>1.065892</td>\n",
       "      <td>0.431913</td>\n",
       "      <td>0.487664</td>\n",
       "      <td>0.926666</td>\n",
       "      <td>0.956709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>1.912431</td>\n",
       "      <td>1.974304</td>\n",
       "      <td>2.019687</td>\n",
       "      <td>1.936474</td>\n",
       "      <td>0.616105</td>\n",
       "      <td>1.110244</td>\n",
       "      <td>1.999673</td>\n",
       "      <td>2.043030</td>\n",
       "      <td>0.478162</td>\n",
       "      <td>0.671980</td>\n",
       "      <td>1.432088</td>\n",
       "      <td>1.459938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>1.826843</td>\n",
       "      <td>1.955649</td>\n",
       "      <td>2.025734</td>\n",
       "      <td>1.943855</td>\n",
       "      <td>0.448129</td>\n",
       "      <td>0.976432</td>\n",
       "      <td>1.986414</td>\n",
       "      <td>2.072413</td>\n",
       "      <td>0.053434</td>\n",
       "      <td>0.398088</td>\n",
       "      <td>1.425848</td>\n",
       "      <td>1.447862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>1.748099</td>\n",
       "      <td>1.918017</td>\n",
       "      <td>2.023138</td>\n",
       "      <td>1.949301</td>\n",
       "      <td>0.334134</td>\n",
       "      <td>1.035781</td>\n",
       "      <td>1.992374</td>\n",
       "      <td>2.059476</td>\n",
       "      <td>0.031406</td>\n",
       "      <td>0.174777</td>\n",
       "      <td>1.411475</td>\n",
       "      <td>1.434117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>1.808605</td>\n",
       "      <td>1.883501</td>\n",
       "      <td>2.023876</td>\n",
       "      <td>1.956944</td>\n",
       "      <td>0.433826</td>\n",
       "      <td>1.023411</td>\n",
       "      <td>1.991311</td>\n",
       "      <td>2.062389</td>\n",
       "      <td>0.303788</td>\n",
       "      <td>0.115146</td>\n",
       "      <td>1.381448</td>\n",
       "      <td>1.416058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>1.792201</td>\n",
       "      <td>1.850873</td>\n",
       "      <td>2.024842</td>\n",
       "      <td>1.963452</td>\n",
       "      <td>0.410703</td>\n",
       "      <td>1.003445</td>\n",
       "      <td>1.989391</td>\n",
       "      <td>2.066845</td>\n",
       "      <td>0.248048</td>\n",
       "      <td>0.181698</td>\n",
       "      <td>1.409682</td>\n",
       "      <td>1.442614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Close       MA5      MA20      MA60     RSI14       std    upperb  \\\n",
       "2265  0.992959  0.999392  1.009366  1.011865  0.654377  0.632325  1.009962   \n",
       "2266  0.962743  0.999293  1.014043  1.017399  0.548284  0.591998  1.008574   \n",
       "2267  0.969075  0.999153  1.017729  1.022714  0.564638  0.572626  1.009220   \n",
       "2268  0.991272  0.997376  1.024353  1.028772  0.619646  0.511912  1.006748   \n",
       "2269  0.971128  0.995125  1.029665  1.034890  0.549769  0.426998  0.999681   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2511  1.912431  1.974304  2.019687  1.936474  0.616105  1.110244  1.999673   \n",
       "2512  1.826843  1.955649  2.025734  1.943855  0.448129  0.976432  1.986414   \n",
       "2513  1.748099  1.918017  2.023138  1.949301  0.334134  1.035781  1.992374   \n",
       "2514  1.808605  1.883501  2.023876  1.956944  0.433826  1.023411  1.991311   \n",
       "2515  1.792201  1.850873  2.024842  1.963452  0.410703  1.003445  1.989391   \n",
       "\n",
       "        lowerb        %K        %D     SP500  NASDAQ_COMP  \n",
       "2265  1.008637  0.708814  0.730503  0.951844     0.992770  \n",
       "2266  1.020645  0.386432  0.583775  0.934195     0.963746  \n",
       "2267  1.028001  0.411229  0.497415  0.925437     0.952094  \n",
       "2268  1.045618  0.634819  0.472111  0.935835     0.972623  \n",
       "2269  1.065892  0.431913  0.487664  0.926666     0.956709  \n",
       "...        ...       ...       ...       ...          ...  \n",
       "2511  2.043030  0.478162  0.671980  1.432088     1.459938  \n",
       "2512  2.072413  0.053434  0.398088  1.425848     1.447862  \n",
       "2513  2.059476  0.031406  0.174777  1.411475     1.434117  \n",
       "2514  2.062389  0.303788  0.115146  1.381448     1.416058  \n",
       "2515  2.066845  0.248048  0.181698  1.409682     1.442614  \n",
       "\n",
       "[251 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(\n",
    "    scaler.transform(test_df),\n",
    "    index=test_df.index,\n",
    "    columns=test_df.columns\n",
    ")\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(input_data:pd.DataFrame, target_column, sequence_length):\n",
    "\n",
    "    sequences = []\n",
    "    data_size = len(input_data)\n",
    "\n",
    "    for i in tqdm(range(data_size - sequence_length)):\n",
    "\n",
    "        sequence = input_data[i:i+sequence_length]\n",
    "\n",
    "        label_position = i + sequence_length\n",
    "        label = input_data.iloc[label_position][target_column]\n",
    "\n",
    "        sequences.append((sequence, label))\n",
    "\n",
    "    return sequences   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc88c2543c3f4d8a870d2b035bb178c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_data = pd.DataFrame(dict(feature_1=[1,2,3,4,5],\n",
    "                                label=[6,7,8,9,10]))\n",
    "\n",
    "sample_sequences = create_sequences(sample_data, 'label', sequence_length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature_1  label\n",
      "1          2      7\n",
      "2          3      8\n",
      "3          4      9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(sample_sequences[1][0])\n",
    "\n",
    "print(sample_sequences[1][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a17126e4d154668a29cf142149329a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2224 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6dd962911847c58852761e342a92d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/211 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>MA5</th>\n",
       "      <th>MA20</th>\n",
       "      <th>MA60</th>\n",
       "      <th>RSI14</th>\n",
       "      <th>std</th>\n",
       "      <th>upperb</th>\n",
       "      <th>lowerb</th>\n",
       "      <th>%K</th>\n",
       "      <th>%D</th>\n",
       "      <th>SP500</th>\n",
       "      <th>NASDAQ_COMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2265</th>\n",
       "      <td>0.992959</td>\n",
       "      <td>0.999392</td>\n",
       "      <td>1.009366</td>\n",
       "      <td>1.011865</td>\n",
       "      <td>0.654377</td>\n",
       "      <td>0.632325</td>\n",
       "      <td>1.009962</td>\n",
       "      <td>1.008637</td>\n",
       "      <td>0.708814</td>\n",
       "      <td>0.730503</td>\n",
       "      <td>0.951844</td>\n",
       "      <td>0.992770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2266</th>\n",
       "      <td>0.962743</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>1.014043</td>\n",
       "      <td>1.017399</td>\n",
       "      <td>0.548284</td>\n",
       "      <td>0.591998</td>\n",
       "      <td>1.008574</td>\n",
       "      <td>1.020645</td>\n",
       "      <td>0.386432</td>\n",
       "      <td>0.583775</td>\n",
       "      <td>0.934195</td>\n",
       "      <td>0.963746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2267</th>\n",
       "      <td>0.969075</td>\n",
       "      <td>0.999153</td>\n",
       "      <td>1.017729</td>\n",
       "      <td>1.022714</td>\n",
       "      <td>0.564638</td>\n",
       "      <td>0.572626</td>\n",
       "      <td>1.009220</td>\n",
       "      <td>1.028001</td>\n",
       "      <td>0.411229</td>\n",
       "      <td>0.497415</td>\n",
       "      <td>0.925437</td>\n",
       "      <td>0.952094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2268</th>\n",
       "      <td>0.991272</td>\n",
       "      <td>0.997376</td>\n",
       "      <td>1.024353</td>\n",
       "      <td>1.028772</td>\n",
       "      <td>0.619646</td>\n",
       "      <td>0.511912</td>\n",
       "      <td>1.006748</td>\n",
       "      <td>1.045618</td>\n",
       "      <td>0.634819</td>\n",
       "      <td>0.472111</td>\n",
       "      <td>0.935835</td>\n",
       "      <td>0.972623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2269</th>\n",
       "      <td>0.971128</td>\n",
       "      <td>0.995125</td>\n",
       "      <td>1.029665</td>\n",
       "      <td>1.034890</td>\n",
       "      <td>0.549769</td>\n",
       "      <td>0.426998</td>\n",
       "      <td>0.999681</td>\n",
       "      <td>1.065892</td>\n",
       "      <td>0.431913</td>\n",
       "      <td>0.487664</td>\n",
       "      <td>0.926666</td>\n",
       "      <td>0.956709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2270</th>\n",
       "      <td>0.995575</td>\n",
       "      <td>0.995658</td>\n",
       "      <td>1.034727</td>\n",
       "      <td>1.041081</td>\n",
       "      <td>0.609959</td>\n",
       "      <td>0.385162</td>\n",
       "      <td>0.998432</td>\n",
       "      <td>1.078581</td>\n",
       "      <td>0.678158</td>\n",
       "      <td>0.578946</td>\n",
       "      <td>0.944156</td>\n",
       "      <td>0.973414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>1.010097</td>\n",
       "      <td>1.005304</td>\n",
       "      <td>1.040398</td>\n",
       "      <td>1.047442</td>\n",
       "      <td>0.642961</td>\n",
       "      <td>0.344242</td>\n",
       "      <td>0.997867</td>\n",
       "      <td>1.091785</td>\n",
       "      <td>0.824427</td>\n",
       "      <td>0.643786</td>\n",
       "      <td>0.938813</td>\n",
       "      <td>0.978736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>1.010170</td>\n",
       "      <td>1.013675</td>\n",
       "      <td>1.045641</td>\n",
       "      <td>1.053607</td>\n",
       "      <td>0.643131</td>\n",
       "      <td>0.298139</td>\n",
       "      <td>0.996185</td>\n",
       "      <td>1.105398</td>\n",
       "      <td>0.846640</td>\n",
       "      <td>0.785610</td>\n",
       "      <td>0.949701</td>\n",
       "      <td>0.997412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>1.048845</td>\n",
       "      <td>1.025402</td>\n",
       "      <td>1.051767</td>\n",
       "      <td>1.060197</td>\n",
       "      <td>0.725205</td>\n",
       "      <td>0.334937</td>\n",
       "      <td>1.006943</td>\n",
       "      <td>1.105916</td>\n",
       "      <td>0.933043</td>\n",
       "      <td>0.872772</td>\n",
       "      <td>0.959972</td>\n",
       "      <td>1.016096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274</th>\n",
       "      <td>1.070114</td>\n",
       "      <td>1.045566</td>\n",
       "      <td>1.058860</td>\n",
       "      <td>1.066839</td>\n",
       "      <td>0.763359</td>\n",
       "      <td>0.394167</td>\n",
       "      <td>1.021734</td>\n",
       "      <td>1.103697</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.932813</td>\n",
       "      <td>0.965176</td>\n",
       "      <td>1.020647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <td>1.075883</td>\n",
       "      <td>1.061925</td>\n",
       "      <td>1.064461</td>\n",
       "      <td>1.073104</td>\n",
       "      <td>0.773379</td>\n",
       "      <td>0.467628</td>\n",
       "      <td>1.037159</td>\n",
       "      <td>1.097415</td>\n",
       "      <td>0.946700</td>\n",
       "      <td>0.967030</td>\n",
       "      <td>0.974472</td>\n",
       "      <td>1.027435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>1.080650</td>\n",
       "      <td>1.076296</td>\n",
       "      <td>1.068784</td>\n",
       "      <td>1.079975</td>\n",
       "      <td>0.781918</td>\n",
       "      <td>0.536044</td>\n",
       "      <td>1.050710</td>\n",
       "      <td>1.090579</td>\n",
       "      <td>0.977269</td>\n",
       "      <td>0.982154</td>\n",
       "      <td>0.983825</td>\n",
       "      <td>1.040442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2277</th>\n",
       "      <td>1.067351</td>\n",
       "      <td>1.087944</td>\n",
       "      <td>1.072551</td>\n",
       "      <td>1.086625</td>\n",
       "      <td>0.722031</td>\n",
       "      <td>0.574709</td>\n",
       "      <td>1.059577</td>\n",
       "      <td>1.088177</td>\n",
       "      <td>0.823325</td>\n",
       "      <td>0.921737</td>\n",
       "      <td>0.984752</td>\n",
       "      <td>1.028927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2278</th>\n",
       "      <td>1.063978</td>\n",
       "      <td>1.091027</td>\n",
       "      <td>1.078372</td>\n",
       "      <td>1.092721</td>\n",
       "      <td>0.706829</td>\n",
       "      <td>0.580584</td>\n",
       "      <td>1.065717</td>\n",
       "      <td>1.093607</td>\n",
       "      <td>0.803357</td>\n",
       "      <td>0.872718</td>\n",
       "      <td>0.988771</td>\n",
       "      <td>1.023872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2279</th>\n",
       "      <td>1.033174</td>\n",
       "      <td>1.083502</td>\n",
       "      <td>1.080846</td>\n",
       "      <td>1.097310</td>\n",
       "      <td>0.578747</td>\n",
       "      <td>0.580327</td>\n",
       "      <td>1.067940</td>\n",
       "      <td>1.096382</td>\n",
       "      <td>0.621039</td>\n",
       "      <td>0.750898</td>\n",
       "      <td>0.977061</td>\n",
       "      <td>1.001866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280</th>\n",
       "      <td>1.090894</td>\n",
       "      <td>1.086560</td>\n",
       "      <td>1.087191</td>\n",
       "      <td>1.102541</td>\n",
       "      <td>0.708251</td>\n",
       "      <td>0.614694</td>\n",
       "      <td>1.078558</td>\n",
       "      <td>1.097556</td>\n",
       "      <td>0.962668</td>\n",
       "      <td>0.798549</td>\n",
       "      <td>0.997464</td>\n",
       "      <td>1.029076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>1.091162</td>\n",
       "      <td>1.088701</td>\n",
       "      <td>1.094034</td>\n",
       "      <td>1.107671</td>\n",
       "      <td>0.708766</td>\n",
       "      <td>0.628340</td>\n",
       "      <td>1.086723</td>\n",
       "      <td>1.102796</td>\n",
       "      <td>0.815703</td>\n",
       "      <td>0.802771</td>\n",
       "      <td>0.994438</td>\n",
       "      <td>1.032665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2282</th>\n",
       "      <td>1.102995</td>\n",
       "      <td>1.095962</td>\n",
       "      <td>1.101161</td>\n",
       "      <td>1.112716</td>\n",
       "      <td>0.731964</td>\n",
       "      <td>0.647792</td>\n",
       "      <td>1.095961</td>\n",
       "      <td>1.107364</td>\n",
       "      <td>0.895703</td>\n",
       "      <td>0.896698</td>\n",
       "      <td>0.994184</td>\n",
       "      <td>1.029914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>1.178585</td>\n",
       "      <td>1.119307</td>\n",
       "      <td>1.110714</td>\n",
       "      <td>1.119643</td>\n",
       "      <td>0.846883</td>\n",
       "      <td>0.785056</td>\n",
       "      <td>1.123950</td>\n",
       "      <td>1.094618</td>\n",
       "      <td>0.965595</td>\n",
       "      <td>0.897698</td>\n",
       "      <td>0.998181</td>\n",
       "      <td>1.043011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2284</th>\n",
       "      <td>1.171128</td>\n",
       "      <td>1.147409</td>\n",
       "      <td>1.120821</td>\n",
       "      <td>1.125934</td>\n",
       "      <td>0.818394</td>\n",
       "      <td>0.860744</td>\n",
       "      <td>1.143803</td>\n",
       "      <td>1.092934</td>\n",
       "      <td>0.893105</td>\n",
       "      <td>0.924167</td>\n",
       "      <td>1.001587</td>\n",
       "      <td>1.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2285</th>\n",
       "      <td>1.159174</td>\n",
       "      <td>1.161317</td>\n",
       "      <td>1.129712</td>\n",
       "      <td>1.132586</td>\n",
       "      <td>0.772455</td>\n",
       "      <td>0.904743</td>\n",
       "      <td>1.158098</td>\n",
       "      <td>1.095284</td>\n",
       "      <td>0.826709</td>\n",
       "      <td>0.900574</td>\n",
       "      <td>0.995059</td>\n",
       "      <td>1.044996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>1.159418</td>\n",
       "      <td>1.175221</td>\n",
       "      <td>1.140233</td>\n",
       "      <td>1.139618</td>\n",
       "      <td>0.772851</td>\n",
       "      <td>0.902454</td>\n",
       "      <td>1.167384</td>\n",
       "      <td>1.107288</td>\n",
       "      <td>0.800646</td>\n",
       "      <td>0.844166</td>\n",
       "      <td>0.999720</td>\n",
       "      <td>1.059049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>1.212468</td>\n",
       "      <td>1.197521</td>\n",
       "      <td>1.153252</td>\n",
       "      <td>1.147780</td>\n",
       "      <td>0.848895</td>\n",
       "      <td>0.949560</td>\n",
       "      <td>1.185885</td>\n",
       "      <td>1.113670</td>\n",
       "      <td>0.937780</td>\n",
       "      <td>0.859444</td>\n",
       "      <td>1.004814</td>\n",
       "      <td>1.064606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288</th>\n",
       "      <td>1.216062</td>\n",
       "      <td>1.205155</td>\n",
       "      <td>1.165277</td>\n",
       "      <td>1.155257</td>\n",
       "      <td>0.853401</td>\n",
       "      <td>0.993827</td>\n",
       "      <td>1.203079</td>\n",
       "      <td>1.119434</td>\n",
       "      <td>0.909177</td>\n",
       "      <td>0.887645</td>\n",
       "      <td>1.019734</td>\n",
       "      <td>1.072662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2289</th>\n",
       "      <td>1.218971</td>\n",
       "      <td>1.214901</td>\n",
       "      <td>1.178534</td>\n",
       "      <td>1.162927</td>\n",
       "      <td>0.857238</td>\n",
       "      <td>0.989794</td>\n",
       "      <td>1.214620</td>\n",
       "      <td>1.134756</td>\n",
       "      <td>0.923231</td>\n",
       "      <td>0.929566</td>\n",
       "      <td>1.025130</td>\n",
       "      <td>1.082951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290</th>\n",
       "      <td>1.221220</td>\n",
       "      <td>1.227540</td>\n",
       "      <td>1.190604</td>\n",
       "      <td>1.170605</td>\n",
       "      <td>0.860368</td>\n",
       "      <td>0.989613</td>\n",
       "      <td>1.225617</td>\n",
       "      <td>1.148113</td>\n",
       "      <td>0.934097</td>\n",
       "      <td>0.928307</td>\n",
       "      <td>1.040482</td>\n",
       "      <td>1.106505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291</th>\n",
       "      <td>1.207065</td>\n",
       "      <td>1.237245</td>\n",
       "      <td>1.201141</td>\n",
       "      <td>1.178110</td>\n",
       "      <td>0.794356</td>\n",
       "      <td>0.968241</td>\n",
       "      <td>1.232239</td>\n",
       "      <td>1.163373</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>0.913444</td>\n",
       "      <td>1.043027</td>\n",
       "      <td>1.101794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2292</th>\n",
       "      <td>1.257866</td>\n",
       "      <td>1.246493</td>\n",
       "      <td>1.214390</td>\n",
       "      <td>1.186631</td>\n",
       "      <td>0.868971</td>\n",
       "      <td>0.978217</td>\n",
       "      <td>1.245739</td>\n",
       "      <td>1.176309</td>\n",
       "      <td>0.999894</td>\n",
       "      <td>0.939660</td>\n",
       "      <td>1.053286</td>\n",
       "      <td>1.110131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2293</th>\n",
       "      <td>1.280039</td>\n",
       "      <td>1.259525</td>\n",
       "      <td>1.226757</td>\n",
       "      <td>1.195294</td>\n",
       "      <td>0.895357</td>\n",
       "      <td>1.030381</td>\n",
       "      <td>1.264354</td>\n",
       "      <td>1.181111</td>\n",
       "      <td>0.923378</td>\n",
       "      <td>0.935994</td>\n",
       "      <td>1.049919</td>\n",
       "      <td>1.119599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>1.323701</td>\n",
       "      <td>1.280859</td>\n",
       "      <td>1.240322</td>\n",
       "      <td>1.205049</td>\n",
       "      <td>0.939471</td>\n",
       "      <td>1.132024</td>\n",
       "      <td>1.291008</td>\n",
       "      <td>1.178840</td>\n",
       "      <td>0.934515</td>\n",
       "      <td>0.959522</td>\n",
       "      <td>1.061436</td>\n",
       "      <td>1.139075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2295</th>\n",
       "      <td>1.375089</td>\n",
       "      <td>1.312202</td>\n",
       "      <td>1.256327</td>\n",
       "      <td>1.215358</td>\n",
       "      <td>0.979964</td>\n",
       "      <td>1.276981</td>\n",
       "      <td>1.325969</td>\n",
       "      <td>1.171913</td>\n",
       "      <td>0.884290</td>\n",
       "      <td>0.919989</td>\n",
       "      <td>1.085132</td>\n",
       "      <td>1.152925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296</th>\n",
       "      <td>1.244933</td>\n",
       "      <td>1.319916</td>\n",
       "      <td>1.265115</td>\n",
       "      <td>1.222454</td>\n",
       "      <td>0.607326</td>\n",
       "      <td>1.254732</td>\n",
       "      <td>1.330871</td>\n",
       "      <td>1.185391</td>\n",
       "      <td>0.411840</td>\n",
       "      <td>0.745059</td>\n",
       "      <td>1.030132</td>\n",
       "      <td>1.081958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>1.206503</td>\n",
       "      <td>1.309453</td>\n",
       "      <td>1.272558</td>\n",
       "      <td>1.229896</td>\n",
       "      <td>0.531086</td>\n",
       "      <td>1.196673</td>\n",
       "      <td>1.329520</td>\n",
       "      <td>1.203461</td>\n",
       "      <td>0.303690</td>\n",
       "      <td>0.529336</td>\n",
       "      <td>1.017845</td>\n",
       "      <td>1.064764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>1.137123</td>\n",
       "      <td>1.280341</td>\n",
       "      <td>1.276471</td>\n",
       "      <td>1.235692</td>\n",
       "      <td>0.415025</td>\n",
       "      <td>1.138094</td>\n",
       "      <td>1.324871</td>\n",
       "      <td>1.217722</td>\n",
       "      <td>0.069065</td>\n",
       "      <td>0.250555</td>\n",
       "      <td>0.976252</td>\n",
       "      <td>1.009560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>1.215548</td>\n",
       "      <td>1.258310</td>\n",
       "      <td>1.286226</td>\n",
       "      <td>1.242580</td>\n",
       "      <td>0.531933</td>\n",
       "      <td>1.009416</td>\n",
       "      <td>1.315719</td>\n",
       "      <td>1.250328</td>\n",
       "      <td>0.334409</td>\n",
       "      <td>0.224075</td>\n",
       "      <td>1.005601</td>\n",
       "      <td>1.044414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>1.176115</td>\n",
       "      <td>1.217779</td>\n",
       "      <td>1.290785</td>\n",
       "      <td>1.248887</td>\n",
       "      <td>0.471579</td>\n",
       "      <td>0.945563</td>\n",
       "      <td>1.310920</td>\n",
       "      <td>1.266197</td>\n",
       "      <td>0.200992</td>\n",
       "      <td>0.188956</td>\n",
       "      <td>0.979466</td>\n",
       "      <td>1.018087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>1.161716</td>\n",
       "      <td>1.200828</td>\n",
       "      <td>1.294559</td>\n",
       "      <td>1.254583</td>\n",
       "      <td>0.450236</td>\n",
       "      <td>0.880384</td>\n",
       "      <td>1.305219</td>\n",
       "      <td>1.281424</td>\n",
       "      <td>0.152274</td>\n",
       "      <td>0.217411</td>\n",
       "      <td>0.980244</td>\n",
       "      <td>1.010255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>1.230925</td>\n",
       "      <td>1.205803</td>\n",
       "      <td>1.301402</td>\n",
       "      <td>1.261690</td>\n",
       "      <td>0.546721</td>\n",
       "      <td>0.807271</td>\n",
       "      <td>1.301206</td>\n",
       "      <td>1.301386</td>\n",
       "      <td>0.386435</td>\n",
       "      <td>0.235203</td>\n",
       "      <td>0.998859</td>\n",
       "      <td>1.034344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2303</th>\n",
       "      <td>1.242538</td>\n",
       "      <td>1.227276</td>\n",
       "      <td>1.304823</td>\n",
       "      <td>1.268943</td>\n",
       "      <td>0.561573</td>\n",
       "      <td>0.800232</td>\n",
       "      <td>1.303342</td>\n",
       "      <td>1.306358</td>\n",
       "      <td>0.425724</td>\n",
       "      <td>0.312054</td>\n",
       "      <td>1.006581</td>\n",
       "      <td>1.050198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2304</th>\n",
       "      <td>1.195942</td>\n",
       "      <td>1.223282</td>\n",
       "      <td>1.306151</td>\n",
       "      <td>1.274754</td>\n",
       "      <td>0.486614</td>\n",
       "      <td>0.788199</td>\n",
       "      <td>1.302866</td>\n",
       "      <td>1.309865</td>\n",
       "      <td>0.268073</td>\n",
       "      <td>0.351653</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>1.033611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Close       MA5      MA20      MA60     RSI14       std    upperb  \\\n",
       "2265  0.992959  0.999392  1.009366  1.011865  0.654377  0.632325  1.009962   \n",
       "2266  0.962743  0.999293  1.014043  1.017399  0.548284  0.591998  1.008574   \n",
       "2267  0.969075  0.999153  1.017729  1.022714  0.564638  0.572626  1.009220   \n",
       "2268  0.991272  0.997376  1.024353  1.028772  0.619646  0.511912  1.006748   \n",
       "2269  0.971128  0.995125  1.029665  1.034890  0.549769  0.426998  0.999681   \n",
       "2270  0.995575  0.995658  1.034727  1.041081  0.609959  0.385162  0.998432   \n",
       "2271  1.010097  1.005304  1.040398  1.047442  0.642961  0.344242  0.997867   \n",
       "2272  1.010170  1.013675  1.045641  1.053607  0.643131  0.298139  0.996185   \n",
       "2273  1.048845  1.025402  1.051767  1.060197  0.725205  0.334937  1.006943   \n",
       "2274  1.070114  1.045566  1.058860  1.066839  0.763359  0.394167  1.021734   \n",
       "2275  1.075883  1.061925  1.064461  1.073104  0.773379  0.467628  1.037159   \n",
       "2276  1.080650  1.076296  1.068784  1.079975  0.781918  0.536044  1.050710   \n",
       "2277  1.067351  1.087944  1.072551  1.086625  0.722031  0.574709  1.059577   \n",
       "2278  1.063978  1.091027  1.078372  1.092721  0.706829  0.580584  1.065717   \n",
       "2279  1.033174  1.083502  1.080846  1.097310  0.578747  0.580327  1.067940   \n",
       "2280  1.090894  1.086560  1.087191  1.102541  0.708251  0.614694  1.078558   \n",
       "2281  1.091162  1.088701  1.094034  1.107671  0.708766  0.628340  1.086723   \n",
       "2282  1.102995  1.095962  1.101161  1.112716  0.731964  0.647792  1.095961   \n",
       "2283  1.178585  1.119307  1.110714  1.119643  0.846883  0.785056  1.123950   \n",
       "2284  1.171128  1.147409  1.120821  1.125934  0.818394  0.860744  1.143803   \n",
       "2285  1.159174  1.161317  1.129712  1.132586  0.772455  0.904743  1.158098   \n",
       "2286  1.159418  1.175221  1.140233  1.139618  0.772851  0.902454  1.167384   \n",
       "2287  1.212468  1.197521  1.153252  1.147780  0.848895  0.949560  1.185885   \n",
       "2288  1.216062  1.205155  1.165277  1.155257  0.853401  0.993827  1.203079   \n",
       "2289  1.218971  1.214901  1.178534  1.162927  0.857238  0.989794  1.214620   \n",
       "2290  1.221220  1.227540  1.190604  1.170605  0.860368  0.989613  1.225617   \n",
       "2291  1.207065  1.237245  1.201141  1.178110  0.794356  0.968241  1.232239   \n",
       "2292  1.257866  1.246493  1.214390  1.186631  0.868971  0.978217  1.245739   \n",
       "2293  1.280039  1.259525  1.226757  1.195294  0.895357  1.030381  1.264354   \n",
       "2294  1.323701  1.280859  1.240322  1.205049  0.939471  1.132024  1.291008   \n",
       "2295  1.375089  1.312202  1.256327  1.215358  0.979964  1.276981  1.325969   \n",
       "2296  1.244933  1.319916  1.265115  1.222454  0.607326  1.254732  1.330871   \n",
       "2297  1.206503  1.309453  1.272558  1.229896  0.531086  1.196673  1.329520   \n",
       "2298  1.137123  1.280341  1.276471  1.235692  0.415025  1.138094  1.324871   \n",
       "2299  1.215548  1.258310  1.286226  1.242580  0.531933  1.009416  1.315719   \n",
       "2300  1.176115  1.217779  1.290785  1.248887  0.471579  0.945563  1.310920   \n",
       "2301  1.161716  1.200828  1.294559  1.254583  0.450236  0.880384  1.305219   \n",
       "2302  1.230925  1.205803  1.301402  1.261690  0.546721  0.807271  1.301206   \n",
       "2303  1.242538  1.227276  1.304823  1.268943  0.561573  0.800232  1.303342   \n",
       "2304  1.195942  1.223282  1.306151  1.274754  0.486614  0.788199  1.302866   \n",
       "\n",
       "        lowerb        %K        %D     SP500  NASDAQ_COMP  \n",
       "2265  1.008637  0.708814  0.730503  0.951844     0.992770  \n",
       "2266  1.020645  0.386432  0.583775  0.934195     0.963746  \n",
       "2267  1.028001  0.411229  0.497415  0.925437     0.952094  \n",
       "2268  1.045618  0.634819  0.472111  0.935835     0.972623  \n",
       "2269  1.065892  0.431913  0.487664  0.926666     0.956709  \n",
       "2270  1.078581  0.678158  0.578946  0.944156     0.973414  \n",
       "2271  1.091785  0.824427  0.643786  0.938813     0.978736  \n",
       "2272  1.105398  0.846640  0.785610  0.949701     0.997412  \n",
       "2273  1.105916  0.933043  0.872772  0.959972     1.016096  \n",
       "2274  1.103697  1.000000  0.932813  0.965176     1.020647  \n",
       "2275  1.097415  0.946700  0.967030  0.974472     1.027435  \n",
       "2276  1.090579  0.977269  0.982154  0.983825     1.040442  \n",
       "2277  1.088177  0.823325  0.921737  0.984752     1.028927  \n",
       "2278  1.093607  0.803357  0.872718  0.988771     1.023872  \n",
       "2279  1.096382  0.621039  0.750898  0.977061     1.001866  \n",
       "2280  1.097556  0.962668  0.798549  0.997464     1.029076  \n",
       "2281  1.102796  0.815703  0.802771  0.994438     1.032665  \n",
       "2282  1.107364  0.895703  0.896698  0.994184     1.029914  \n",
       "2283  1.094618  0.965595  0.897698  0.998181     1.043011  \n",
       "2284  1.092934  0.893105  0.924167  1.001587     1.052632  \n",
       "2285  1.095284  0.826709  0.900574  0.995059     1.044996  \n",
       "2286  1.107288  0.800646  0.844166  0.999720     1.059049  \n",
       "2287  1.113670  0.937780  0.859444  1.004814     1.064606  \n",
       "2288  1.119434  0.909177  0.887645  1.019734     1.072662  \n",
       "2289  1.134756  0.923231  0.929566  1.025130     1.082951  \n",
       "2290  1.148113  0.934097  0.928307  1.040482     1.106505  \n",
       "2291  1.163373  0.865714  0.913444  1.043027     1.101794  \n",
       "2292  1.176309  0.999894  0.939660  1.053286     1.110131  \n",
       "2293  1.181111  0.923378  0.935994  1.049919     1.119599  \n",
       "2294  1.178840  0.934515  0.959522  1.061436     1.139075  \n",
       "2295  1.171913  0.884290  0.919989  1.085132     1.152925  \n",
       "2296  1.185391  0.411840  0.745059  1.030132     1.081958  \n",
       "2297  1.203461  0.303690  0.529336  1.017845     1.064764  \n",
       "2298  1.217722  0.069065  0.250555  0.976252     1.009560  \n",
       "2299  1.250328  0.334409  0.224075  1.005601     1.044414  \n",
       "2300  1.266197  0.200992  0.188956  0.979466     1.018087  \n",
       "2301  1.281424  0.152274  0.217411  0.980244     1.010255  \n",
       "2302  1.301386  0.386435  0.235203  0.998859     1.034344  \n",
       "2303  1.306358  0.425724  0.312054  1.006581     1.050198  \n",
       "2304  1.309865  0.268073  0.351653  0.999711     1.033611  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEQUENCE_LENGTH = 40\n",
    "\n",
    "train_sequences = create_sequences(train_df, 'Close', SEQUENCE_LENGTH)\n",
    "test_sequences = create_sequences(test_df, 'Close', SEQUENCE_LENGTH)\n",
    "\n",
    "test_sequences[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class StockDataset(Dataset):\n",
    "    \n",
    "#     def __init__(self, sequences):\n",
    "#         self.sequences = sequences\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.sequences)\n",
    "\n",
    "#     def __getitem__(self, idx):  # Dataset contains tuple (data, label)\n",
    "#         sequence, label = self.sequences[idx]\n",
    "\n",
    "#         return dict(\n",
    "#             sequence = torch.Tensor(sequence.to_numpy()),\n",
    "#             label = torch.tensor(label).float()\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class StockPriceDataModule(L.LightningDataModule):\n",
    "\n",
    "#     def __init__(self, train_sequences, test_sequences, batch_size=8):\n",
    "#         super().__init__()\n",
    "#         self.train_sequences = train_sequences\n",
    "#         self.test_sequences = test_sequences\n",
    "#         self.batch_size = batch_size\n",
    "\n",
    "#     def setup(self):\n",
    "#         self.train_dataset = StockDataset(self.train_sequences)\n",
    "#         self.test_dataset = StockDataset(self.test_sequences)\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         return DataLoader(\n",
    "#             self.train_dataset,\n",
    "#             batch_size = self.batch_size,\n",
    "#             shuffle = False,\n",
    "#             num_workers = 2                  # making it faster?\n",
    "#         )\n",
    "    \n",
    "#     def val_dataloader(self):\n",
    "#         return DataLoader(\n",
    "#             self.test_dataset,\n",
    "#             batch_size = 1,\n",
    "#             shuffle = False,\n",
    "#             num_workers = 1\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 8\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_module = StockPriceDataModule(train_sequences, test_sequences, batch_size=BATCH_SIZE)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = StockDataset(train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 12])\n",
      "torch.Size([])\n",
      "tensor(0.0100)\n"
     ]
    }
   ],
   "source": [
    "for item in train_dataset:\n",
    "    print(item['sequence'].shape)\n",
    "    print(item['label'].shape)\n",
    "    print(item['label'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2264, 12)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PricePredictionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_features, n_hidden=128, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = n_features, \n",
    "            hidden_size = n_hidden,\n",
    "            batch_first = True,        # batch_size as the first parameter\n",
    "            num_layers = n_layers,\n",
    "            dropout = 0.2\n",
    "        )\n",
    "\n",
    "        self.regressor = nn.Linear(n_hidden, 1)  # output line\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.lstm.flatten_parameters()\n",
    "\n",
    "        _, (hidden, _) = self.lstm(x)   # see the  document for reference\n",
    "        out = hidden[-1]\n",
    "\n",
    "        return self.regressor(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPricePredictor(L.LightningModule):\n",
    "\n",
    "    def __init__(self, n_features: int):\n",
    "        super().__init__()\n",
    "        self.model = PricePredictionModel(n_features)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        output = self.model(x)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels.unsqueeze(dim=1))\n",
    "        return loss, output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        sequences = batch[\"sequence\"]\n",
    "        labels = batch[\"label\"]\n",
    "        loss, outputs = self(sequences, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        sequences = batch[\"sequence\"]\n",
    "        labels = batch[\"label\"]\n",
    "        loss, outputs = self(sequences, labels)\n",
    "        self.log(\"validation_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        sequences = batch[\"sequence\"]\n",
    "        labels = batch[\"label\"]\n",
    "        loss, outputs = self(sequences, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(self.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StockPricePredictor(n_features=train_df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Imported\n",
      "Successfully Imported\n",
      "torch.Size([64, 40, 12])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for item in data_module.train_dataloader():\n",
    "    print(item['sequence'].shape)\n",
    "    print(item['label'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0 사용 가능합니다\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath = \"checkpoints\",\n",
    "    filename = \"best checkpoint\",\n",
    "    save_top_k = 1,\n",
    "    verbose= True,\n",
    "    monitor = 'val_loss',\n",
    "    mode = \"min\"          # minimum value\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name='stock-price')\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss', patience=2)\n",
    "\n",
    "device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"{device} 사용 가능합니다\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=[early_stopping_callback, checkpoint_callback],\n",
    "    max_epochs=N_EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type                 | Params\n",
      "---------------------------------------------------\n",
      "0 | model     | PricePredictionModel | 204 K \n",
      "1 | criterion | MSELoss              | 0     \n",
      "---------------------------------------------------\n",
      "204 K     Trainable params\n",
      "0         Non-trainable params\n",
      "204 K     Total params\n",
      "0.820     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4a6c9f32be4e8798d06cc3cce24a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sihun/anaconda3/envs/madelion/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sihun/anaconda3/envs/madelion/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Imported\n",
      "Successfully Imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sihun/anaconda3/envs/madelion/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (35) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec39ca884dac44b5a00142bcef5e4637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Imported\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee59e7284084094b1cdd387b28b6290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Early stopping conditioned on metric `val_loss` which is not available. Pass in or modify your `EarlyStopping` callback to use any of the following: `train_loss`, `validation_loss`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/sihun/Desktop/Github/LTP1.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sihun/Desktop/Github/LTP1.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, data_module)\n",
      "File \u001b[0;32m~/anaconda3/envs/madelion/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    545\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    546\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/madelion/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/anaconda3/envs/madelion/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    582\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/madelion/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    986\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    991\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    994\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/madelion/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1035\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1034\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1035\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1036\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/madelion/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:203\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance()\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/madelion/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:374\u001b[0m, in \u001b[0;36m_FitLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    372\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(trainer, \u001b[39m\"\u001b[39m\u001b[39mon_train_epoch_end\u001b[39m\u001b[39m\"\u001b[39m, monitoring_callbacks\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    373\u001b[0m call\u001b[39m.\u001b[39m_call_lightning_module_hook(trainer, \u001b[39m\"\u001b[39m\u001b[39mon_train_epoch_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 374\u001b[0m call\u001b[39m.\u001b[39;49m_call_callback_hooks(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mon_train_epoch_end\u001b[39;49m\u001b[39m\"\u001b[39;49m, monitoring_callbacks\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    376\u001b[0m trainer\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39mon_epoch_end()\n\u001b[1;32m    378\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch_loop\u001b[39m.\u001b[39m_num_ready_batches_reached():\n\u001b[1;32m    379\u001b[0m     \u001b[39m# if we are restarting and the above condition holds, it's because we are reloading an epoch-end checkpoint.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[39m# since metric-based schedulers require access to metrics and those are not currently saved in the\u001b[39;00m\n\u001b[1;32m    381\u001b[0m     \u001b[39m# checkpoint, the plateau schedulers shouldn't be updated\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/madelion/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:208\u001b[0m, in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(fn):\n\u001b[1;32m    207\u001b[0m         \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Callback]\u001b[39m\u001b[39m{\u001b[39;00mcallback\u001b[39m.\u001b[39mstate_key\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 208\u001b[0m             fn(trainer, trainer\u001b[39m.\u001b[39;49mlightning_module, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mif\u001b[39;00m pl_module:\n\u001b[1;32m    211\u001b[0m     \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/anaconda3/envs/madelion/lib/python3.11/site-packages/lightning/pytorch/callbacks/early_stopping.py:183\u001b[0m, in \u001b[0;36mEarlyStopping.on_train_epoch_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_on_train_epoch_end \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_skip_check(trainer):\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_early_stopping_check(trainer)\n",
      "File \u001b[0;32m~/anaconda3/envs/madelion/lib/python3.11/site-packages/lightning/pytorch/callbacks/early_stopping.py:194\u001b[0m, in \u001b[0;36mEarlyStopping._run_early_stopping_check\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Checks whether the early stopping condition is met and if so tells the trainer to stop the training.\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m logs \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mcallback_metrics\n\u001b[0;32m--> 194\u001b[0m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mfast_dev_run \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_condition_metric(  \u001b[39m# disable early_stopping with fast_dev_run\u001b[39;49;00m\n\u001b[1;32m    195\u001b[0m     logs\n\u001b[1;32m    196\u001b[0m ):  \u001b[39m# short circuit if metric not present\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    199\u001b[0m current \u001b[39m=\u001b[39m logs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmonitor]\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/anaconda3/envs/madelion/lib/python3.11/site-packages/lightning/pytorch/callbacks/early_stopping.py:149\u001b[0m, in \u001b[0;36mEarlyStopping._validate_condition_metric\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m monitor_val \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrict:\n\u001b[0;32m--> 149\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(error_msg)\n\u001b[1;32m    150\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    151\u001b[0m         rank_zero_warn(error_msg, category\u001b[39m=\u001b[39m\u001b[39mRuntimeWarning\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Early stopping conditioned on metric `val_loss` which is not available. Pass in or modify your `EarlyStopping` callback to use any of the following: `train_loss`, `validation_loss`"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "madelion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
