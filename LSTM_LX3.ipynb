{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0 사용 가능합니다\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas_datareader.data as pd_web\n",
    "from datetime import datetime\n",
    "import yfinance as yf\n",
    "yf.pdr_override()  # update pandas datareader (yahoo finance api)\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더\n",
    "\n",
    "device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"{device} 사용 가능합니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.series.Series, pandas.core.frame.DataFrame)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./NVDA_110721_Final_1_fdr.csv', index_col='Date')\n",
    "df['Close_y'] = df['Close']\n",
    "\n",
    "seq_length = 40\n",
    "batch = 16\n",
    "train_size = int(len(df) * 0.7)\n",
    "train_set = df[0:train_size]\n",
    "test_set = df[train_size - seq_length:]\n",
    "\n",
    "train_set.shape, test_set.shape\n",
    "\n",
    "test_set\n",
    "type(df.iloc[:, -1]), type(df.iloc[:, [-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sihun/anaconda3/envs/madellion/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/sihun/anaconda3/envs/madellion/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/sihun/anaconda3/envs/madellion/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/Users/sihun/anaconda3/envs/madellion/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/sihun/anaconda3/envs/madellion/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/sihun/anaconda3/envs/madellion/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/Users/sihun/anaconda3/envs/madellion/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/sihun/anaconda3/envs/madellion/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/sihun/anaconda3/envs/madellion/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/Users/sihun/anaconda3/envs/madellion/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/sihun/anaconda3/envs/madellion/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/sihun/anaconda3/envs/madellion/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/Users/sihun/anaconda3/envs/madellion/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/sihun/anaconda3/envs/madellion/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/sihun/anaconda3/envs/madellion/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/Users/sihun/anaconda3/envs/madellion/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/sihun/anaconda3/envs/madellion/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/sihun/anaconda3/envs/madellion/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>MA5</th>\n",
       "      <th>MA20</th>\n",
       "      <th>MA60</th>\n",
       "      <th>RSI14</th>\n",
       "      <th>std</th>\n",
       "      <th>upperb</th>\n",
       "      <th>lowerb</th>\n",
       "      <th>%K</th>\n",
       "      <th>%D</th>\n",
       "      <th>SP500</th>\n",
       "      <th>NASDAQ_COMP</th>\n",
       "      <th>Close_y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011/07/21</th>\n",
       "      <td>-0.613332</td>\n",
       "      <td>-0.619564</td>\n",
       "      <td>-0.601599</td>\n",
       "      <td>-0.560204</td>\n",
       "      <td>-1.183925</td>\n",
       "      <td>-0.472505</td>\n",
       "      <td>-0.596549</td>\n",
       "      <td>-0.606209</td>\n",
       "      <td>-0.582667</td>\n",
       "      <td>-1.136944</td>\n",
       "      <td>-1.380350</td>\n",
       "      <td>-1.290105</td>\n",
       "      <td>-0.613332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011/07/22</th>\n",
       "      <td>-0.609155</td>\n",
       "      <td>-0.617047</td>\n",
       "      <td>-0.602458</td>\n",
       "      <td>-0.561271</td>\n",
       "      <td>-0.844317</td>\n",
       "      <td>-0.485486</td>\n",
       "      <td>-0.598451</td>\n",
       "      <td>-0.605842</td>\n",
       "      <td>-0.208151</td>\n",
       "      <td>-0.787392</td>\n",
       "      <td>-1.377541</td>\n",
       "      <td>-1.272344</td>\n",
       "      <td>-0.609155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011/07/25</th>\n",
       "      <td>-0.612497</td>\n",
       "      <td>-0.614362</td>\n",
       "      <td>-0.603154</td>\n",
       "      <td>-0.562450</td>\n",
       "      <td>-1.054733</td>\n",
       "      <td>-0.489928</td>\n",
       "      <td>-0.599473</td>\n",
       "      <td>-0.606152</td>\n",
       "      <td>-0.270569</td>\n",
       "      <td>-0.379580</td>\n",
       "      <td>-1.395019</td>\n",
       "      <td>-1.284013</td>\n",
       "      <td>-0.612497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011/07/26</th>\n",
       "      <td>-0.617509</td>\n",
       "      <td>-0.613859</td>\n",
       "      <td>-0.603871</td>\n",
       "      <td>-0.563840</td>\n",
       "      <td>-1.348803</td>\n",
       "      <td>-0.487934</td>\n",
       "      <td>-0.599966</td>\n",
       "      <td>-0.607131</td>\n",
       "      <td>-0.758910</td>\n",
       "      <td>-0.442847</td>\n",
       "      <td>-1.407662</td>\n",
       "      <td>-1.286080</td>\n",
       "      <td>-0.617509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011/07/27</th>\n",
       "      <td>-0.625026</td>\n",
       "      <td>-0.614362</td>\n",
       "      <td>-0.605036</td>\n",
       "      <td>-0.565297</td>\n",
       "      <td>-1.732635</td>\n",
       "      <td>-0.479145</td>\n",
       "      <td>-0.600293</td>\n",
       "      <td>-0.609277</td>\n",
       "      <td>-1.532441</td>\n",
       "      <td>-0.918239</td>\n",
       "      <td>-1.469955</td>\n",
       "      <td>-1.340796</td>\n",
       "      <td>-0.625026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018/07/13</th>\n",
       "      <td>-0.670242</td>\n",
       "      <td>-0.667866</td>\n",
       "      <td>-0.662168</td>\n",
       "      <td>-0.645826</td>\n",
       "      <td>-4.619692</td>\n",
       "      <td>1.982617</td>\n",
       "      <td>-0.674130</td>\n",
       "      <td>-0.646832</td>\n",
       "      <td>-1.910230</td>\n",
       "      <td>-2.069635</td>\n",
       "      <td>-4.470387</td>\n",
       "      <td>-3.351592</td>\n",
       "      <td>-0.670242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018/07/16</th>\n",
       "      <td>-0.671110</td>\n",
       "      <td>-0.668030</td>\n",
       "      <td>-0.662856</td>\n",
       "      <td>-0.645543</td>\n",
       "      <td>-4.627021</td>\n",
       "      <td>1.665445</td>\n",
       "      <td>-0.675871</td>\n",
       "      <td>-0.646052</td>\n",
       "      <td>-1.916145</td>\n",
       "      <td>-2.058705</td>\n",
       "      <td>-4.470402</td>\n",
       "      <td>-3.351603</td>\n",
       "      <td>-0.671110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018/07/17</th>\n",
       "      <td>-0.666854</td>\n",
       "      <td>-0.667962</td>\n",
       "      <td>-0.663316</td>\n",
       "      <td>-0.645174</td>\n",
       "      <td>-4.591807</td>\n",
       "      <td>1.337446</td>\n",
       "      <td>-0.677457</td>\n",
       "      <td>-0.644948</td>\n",
       "      <td>-1.887972</td>\n",
       "      <td>-2.047674</td>\n",
       "      <td>-4.470343</td>\n",
       "      <td>-3.351576</td>\n",
       "      <td>-0.666854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018/07/18</th>\n",
       "      <td>-0.668397</td>\n",
       "      <td>-0.667309</td>\n",
       "      <td>-0.663657</td>\n",
       "      <td>-0.644762</td>\n",
       "      <td>-4.605934</td>\n",
       "      <td>1.138021</td>\n",
       "      <td>-0.678474</td>\n",
       "      <td>-0.644350</td>\n",
       "      <td>-1.898402</td>\n",
       "      <td>-2.043101</td>\n",
       "      <td>-4.470311</td>\n",
       "      <td>-3.351577</td>\n",
       "      <td>-0.668397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018/07/19</th>\n",
       "      <td>-0.668141</td>\n",
       "      <td>-0.667184</td>\n",
       "      <td>-0.664072</td>\n",
       "      <td>-0.644307</td>\n",
       "      <td>-4.603729</td>\n",
       "      <td>0.820141</td>\n",
       "      <td>-0.679985</td>\n",
       "      <td>-0.643243</td>\n",
       "      <td>-1.896672</td>\n",
       "      <td>-2.035573</td>\n",
       "      <td>-4.470370</td>\n",
       "      <td>-3.351592</td>\n",
       "      <td>-0.668141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1761 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Close       MA5      MA20      MA60     RSI14       std  \\\n",
       "Date                                                                     \n",
       "2011/07/21 -0.613332 -0.619564 -0.601599 -0.560204 -1.183925 -0.472505   \n",
       "2011/07/22 -0.609155 -0.617047 -0.602458 -0.561271 -0.844317 -0.485486   \n",
       "2011/07/25 -0.612497 -0.614362 -0.603154 -0.562450 -1.054733 -0.489928   \n",
       "2011/07/26 -0.617509 -0.613859 -0.603871 -0.563840 -1.348803 -0.487934   \n",
       "2011/07/27 -0.625026 -0.614362 -0.605036 -0.565297 -1.732635 -0.479145   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2018/07/13 -0.670242 -0.667866 -0.662168 -0.645826 -4.619692  1.982617   \n",
       "2018/07/16 -0.671110 -0.668030 -0.662856 -0.645543 -4.627021  1.665445   \n",
       "2018/07/17 -0.666854 -0.667962 -0.663316 -0.645174 -4.591807  1.337446   \n",
       "2018/07/18 -0.668397 -0.667309 -0.663657 -0.644762 -4.605934  1.138021   \n",
       "2018/07/19 -0.668141 -0.667184 -0.664072 -0.644307 -4.603729  0.820141   \n",
       "\n",
       "              upperb    lowerb        %K        %D     SP500  NASDAQ_COMP  \\\n",
       "Date                                                                        \n",
       "2011/07/21 -0.596549 -0.606209 -0.582667 -1.136944 -1.380350    -1.290105   \n",
       "2011/07/22 -0.598451 -0.605842 -0.208151 -0.787392 -1.377541    -1.272344   \n",
       "2011/07/25 -0.599473 -0.606152 -0.270569 -0.379580 -1.395019    -1.284013   \n",
       "2011/07/26 -0.599966 -0.607131 -0.758910 -0.442847 -1.407662    -1.286080   \n",
       "2011/07/27 -0.600293 -0.609277 -1.532441 -0.918239 -1.469955    -1.340796   \n",
       "...              ...       ...       ...       ...       ...          ...   \n",
       "2018/07/13 -0.674130 -0.646832 -1.910230 -2.069635 -4.470387    -3.351592   \n",
       "2018/07/16 -0.675871 -0.646052 -1.916145 -2.058705 -4.470402    -3.351603   \n",
       "2018/07/17 -0.677457 -0.644948 -1.887972 -2.047674 -4.470343    -3.351576   \n",
       "2018/07/18 -0.678474 -0.644350 -1.898402 -2.043101 -4.470311    -3.351577   \n",
       "2018/07/19 -0.679985 -0.643243 -1.896672 -2.035573 -4.470370    -3.351592   \n",
       "\n",
       "             Close_y  \n",
       "Date                  \n",
       "2011/07/21 -0.613332  \n",
       "2011/07/22 -0.609155  \n",
       "2011/07/25 -0.612497  \n",
       "2011/07/26 -0.617509  \n",
       "2011/07/27 -0.625026  \n",
       "...              ...  \n",
       "2018/07/13 -0.670242  \n",
       "2018/07/16 -0.671110  \n",
       "2018/07/17 -0.666854  \n",
       "2018/07/18 -0.668397  \n",
       "2018/07/19 -0.668141  \n",
       "\n",
       "[1761 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input scale\n",
    "scaler_x = StandardScaler()\n",
    "scaler_x.fit(train_set.iloc[:, :-1])  # 맨마지막에 test y 추가\n",
    "\n",
    "train_set.iloc[:, :-1] = scaler_x.transform(train_set.iloc[:, :-1])\n",
    "test_set.iloc[:, :-1] = scaler_x.transform(test_set.iloc[:, :-1])\n",
    "\n",
    "# Output scale\n",
    "scaler_y = StandardScaler()\n",
    "scaler_y.fit(train_set.iloc[:, [-1]])\n",
    "\n",
    "train_set.iloc[:, -1] = scaler_y.transform(train_set.iloc[:, [-1]])\n",
    "test_set.iloc[:, -1] = scaler_y.transform(test_set.iloc[:, [-1]])\n",
    "\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성 함수\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(time_series)-seq_length):\n",
    "        _x = time_series[i:i+seq_length, :]\n",
    "        _y = time_series[i+seq_length, [-1]]\n",
    "        # print(_x, \"-->\",_y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainX, trainY = build_dataset(np.array(train_set), seq_length)\n",
    "testX, testY = build_dataset(np.array(test_set), seq_length)\n",
    "\n",
    "trainX_tensor = torch.tensor(trainX)\n",
    "trainY_tensor = torch.tensor(trainY)\n",
    "\n",
    "testX_tensor = torch.tensor(testX)\n",
    "testY_tensor = torch.tensor(testY)\n",
    "\n",
    "dataset = TensorDataset(trainX_tensor, trainY_tensor)\n",
    "\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=batch,\n",
    "                        shuffle=True,\n",
    "                        drop_last=True)  # 싸이즈 다르면 드롭?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정값\n",
    "data_dim = 5\n",
    "hidden_dim = 10 \n",
    "output_dim = 1 \n",
    "learning_rate = 0.01\n",
    "nb_epochs = 100\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, seq_len, output_dim, layers):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.output_dim = output_dim\n",
    "        self.layers = layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, \n",
    "                            hidden_dim,\n",
    "                            num_layers=layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim, bias = True)\n",
    "\n",
    "    def reset_hidden_state(self): \n",
    "        self.hidden = (\n",
    "                torch.zeros(self.layers, self.seq_len, self.hidden_dim),\n",
    "                torch.zeros(self.layers, self.seq_len, self.hidden_dim))\n",
    "        \n",
    "    # 예측을 위한 함수\n",
    "    def forward(self, x):\n",
    "        x, _status = self.lstm(x)\n",
    "        x = self.fc(x[:, -1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_df, num_epochs = None, lr = None, verbose = 10, patience = 10):\n",
    "     \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    nb_epochs = num_epochs\n",
    "    \n",
    "    # epoch마다 loss 저장\n",
    "    train_hist = np.zeros(nb_epochs)\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = len(train_df)\n",
    "        \n",
    "        for batch_idx, samples in enumerate(train_df):\n",
    "\n",
    "            x_train, y_train = samples\n",
    "            \n",
    "            # seq별 hidden state reset\n",
    "            model.reset_hidden_state()\n",
    "            \n",
    "            # H(x) 계산\n",
    "            outputs = model(x_train)\n",
    "                \n",
    "            # cost 계산\n",
    "            loss = criterion(outputs, y_train)                    \n",
    "            \n",
    "            # cost로 H(x) 개선\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_cost += loss/total_batch\n",
    "               \n",
    "        train_hist[epoch] = avg_cost        \n",
    "        \n",
    "        if epoch % verbose == 0:\n",
    "            print('Epoch:', '%04d' % (epoch), 'train loss :', '{:.4f}'.format(avg_cost))\n",
    "            \n",
    "        # patience번째 마다 early stopping 여부 확인\n",
    "        if (epoch % patience == 0) & (epoch != 0):\n",
    "            \n",
    "            # loss가 커졌다면 early stop\n",
    "            if train_hist[epoch-patience] < train_hist[epoch]:\n",
    "                print('\\n Early Stopping')\n",
    "                \n",
    "                break\n",
    "            \n",
    "    return model.eval(), train_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (640x13 and 5x40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m net \u001b[38;5;241m=\u001b[39m Net(data_dim, hidden_dim, seq_length, output_dim, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model, train_hist \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnb_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_df, num_epochs, lr, verbose, patience)\u001b[0m\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mreset_hidden_state()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# H(x) 계산\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# cost 계산\u001b[39;00m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train)                    \n",
      "File \u001b[0;32m~/anaconda3/envs/madellion/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/madellion/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 30\u001b[0m     x, _status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/madellion/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/madellion/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/madellion/lib/python3.11/site-packages/torch/nn/modules/rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    876\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    882\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    883\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (640x13 and 5x40)"
     ]
    }
   ],
   "source": [
    "net = Net(data_dim, hidden_dim, seq_length, output_dim, 1)\n",
    "model, train_hist = train_model(net, dataloader, num_epochs = nb_epochs, lr = learning_rate, verbose = 20, patience = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "madellion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
